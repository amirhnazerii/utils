{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Simple word-level encoder-decoder from scratch\n",
        "\n",
        "Credit to Amir Nazeri & \\\n",
        "Udacity Gen AI nanodegree\n"
      ],
      "metadata": {
        "id": "VJ9u3IUJqGBw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": [],
        "id": "fkxhH84wVL3w"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "# COMPLETE: Feel free to add other imports as needed\n",
        "import string\n",
        "import re\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYUCplf0VL30"
      },
      "source": [
        "# Tokenization Steps\n",
        "\n",
        "In this exercise, you'll code your own tokenizer from scratching using base\n",
        "Python!\n",
        "\n",
        "You might normally start with a pretrained tokenizer, but this exercise will\n",
        "help you get to know see some of the tokenization steps better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_b-a7ZXVL31"
      },
      "source": [
        "## Define Sample Text\n",
        "\n",
        "Let's first define some sample text you will use to test your tokenization\n",
        "steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2qSLjOSVL32",
        "outputId": "3851feeb-25eb-4d8b-d5f2-dc9a03ba3d59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mr. Louis continued to say, \"Penguins are important, \n",
            "but we mustn't forget the nuumber 1 priority: the READER!\"\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sample_text = '''Mr. Louis continued to say, \"Penguins are important,\n",
        "but we mustn't forget the nuumber 1 priority: the READER!\"\n",
        "'''\n",
        "\n",
        "print(sample_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGYym1iQVL33"
      },
      "source": [
        "## Normalization\n",
        "\n",
        "This step is where you'll normalize your text by converting to lowercase,\n",
        "removing accented characters, etc.\n",
        "\n",
        "For example, the text:\n",
        "```\n",
        "Did Uncle Max like the jalapeño dip?\n",
        "```\n",
        "might be normalized to:\n",
        "```\n",
        "did uncle max like the jalapeno dip\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "tags": [],
        "id": "N-whGB0rVL33"
      },
      "outputs": [],
      "source": [
        "def normalize_text(text: str) -> str:\n",
        "    # Normalize incoming text; can be multiple actions\n",
        "    # Only keep ASCII letters, numbers, punctuation, and whitespace characters\n",
        "    acceptable_characters = (\n",
        "        string.ascii_letters\n",
        "        + string.digits\n",
        "        + string.punctuation\n",
        "        + string.whitespace\n",
        "    )\n",
        "    normalized_text = ''.join(\n",
        "        filter(lambda letter: letter in acceptable_characters, text)\n",
        "    )\n",
        "    # Make text lower-case\n",
        "    normalized_text = normalized_text.lower()\n",
        "    return normalized_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eDxgniCoVL34",
        "outputId": "21a57285-ab9b-421a-9ac9-095cb1adfbc6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mr. louis continued to say, \"penguins are important, \\nbut we mustn\\'t forget the nuumber 1 priority: the reader!\"\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Test out your normalization\n",
        "normalize_text(sample_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZyLsESpVL34"
      },
      "source": [
        "## Pretokenization\n",
        "\n",
        "This step will take in the normalized text and pretokenize the text into a list\n",
        "of smaller pieces.\n",
        "\n",
        "For example, the text:\n",
        "```\n",
        "Did Uncle Max like the jalapeño dip?\n",
        "```\n",
        "might be normalized & then pretokenized to:\n",
        "```\n",
        "[\n",
        "    'did',\n",
        "    'uncle',\n",
        "    'max',\n",
        "    'like',\n",
        "    'the',\n",
        "    'jalapeno',\n",
        "    'dip?',\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QMZTzVVeVL34"
      },
      "outputs": [],
      "source": [
        "def pretokenize_text(text: str) -> list[str]:\n",
        "    # pretokenize normalized text\n",
        "    # Split based on spaces\n",
        "    smaller_pieces = text.split()\n",
        "    return smaller_pieces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLIvBAHEVL35",
        "outputId": "1c87bf14-3059-4b19-ebbc-6bb724b389a5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mr.',\n",
              " 'louis',\n",
              " 'continued',\n",
              " 'to',\n",
              " 'say,',\n",
              " '\"penguins',\n",
              " 'are',\n",
              " 'important,',\n",
              " 'but',\n",
              " 'we',\n",
              " \"mustn't\",\n",
              " 'forget',\n",
              " 'the',\n",
              " 'nuumber',\n",
              " '1',\n",
              " 'priority:',\n",
              " 'the',\n",
              " 'reader!\"']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Test out your pretokenization step (after normalizing the text)\n",
        "normalized_text = normalize_text(sample_text)\n",
        "pretokenize_text(normalized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_d0EHX5VL35"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "This step will take in the list of pretokenized pieces (after the text has\n",
        "been normalized) into the tokens that will be used.\n",
        "\n",
        "For example, the text:\n",
        "```\n",
        "Did Uncle Max like the jalapeño dip?\n",
        "```\n",
        "might be normalized, pretokenized, and then tokenized to:\n",
        "```\n",
        "[\n",
        "    'did',\n",
        "    'uncle',\n",
        "    'max',\n",
        "    'like',\n",
        "    'the',\n",
        "    'jalapeno',\n",
        "    'dip'\n",
        "    '?',\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WbTSODsaVL35"
      },
      "outputs": [],
      "source": [
        "# Combine normalization and pretokenization steps before breaking things further\n",
        "def tokenize_text(text: str) -> list[str]:\n",
        "    # Apply created steps\n",
        "    normalized_text: str = normalize_text(text)\n",
        "    pretokenized_text: list[str] = pretokenize_text(normalized_text)\n",
        "    # COMPLETE: Go through pretokenized text to create a list of tokens\n",
        "    tokens = []\n",
        "    # Small 'pieces' to make full tokens\n",
        "    for word in pretokenized_text:\n",
        "        tokens.extend(\n",
        "            re.findall(\n",
        "                f'[\\w]+|[{string.punctuation}]', # Split word at punctuations\n",
        "                word,\n",
        "            )\n",
        "        )\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W92pesOVVL35",
        "outputId": "dd1c7de7-5a7f-42c6-8325-65f35f9aa5ef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mr',\n",
              " '.',\n",
              " 'louis',\n",
              " 'continued',\n",
              " 'to',\n",
              " 'say',\n",
              " ',',\n",
              " '\"',\n",
              " 'penguins',\n",
              " 'are',\n",
              " 'important',\n",
              " ',',\n",
              " 'but',\n",
              " 'we',\n",
              " 'mustn',\n",
              " \"'\",\n",
              " 't',\n",
              " 'forget',\n",
              " 'the',\n",
              " 'nuumber',\n",
              " '1',\n",
              " 'priority',\n",
              " ':',\n",
              " 'the',\n",
              " 'reader',\n",
              " '!',\n",
              " '\"']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Test out your tokenization (that uses normalizing & pretokenizing functions)\n",
        "tokenize_text(sample_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmuDZ2E1VL35"
      },
      "source": [
        "## Postprocessing\n",
        "\n",
        "This final step will take in the list of tokens from the original text and add\n",
        "any special tokens to the text.\n",
        "\n",
        "For example, the text:\n",
        "```\n",
        "Did Uncle Max like the jalapeño dip?\n",
        "```\n",
        "might be normalized, pretokenized, and then tokenized to:\n",
        "```\n",
        "[\n",
        "    '[BOS]',\n",
        "    'did',\n",
        "    'uncle',\n",
        "    'max',\n",
        "    'like',\n",
        "    'the',\n",
        "    'jalapeno',\n",
        "    'dip'\n",
        "    '?',\n",
        "    '[EOS]',\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "gRCLYjTxVL36"
      },
      "outputs": [],
      "source": [
        "# Useful for some tasks\n",
        "def postprocess_tokens(tokens: list[str]) -> list[str]:\n",
        "    # COMPLETE: Add beginning and end of sequence tokens to your tokenized text\n",
        "    # Can use a format like '[BOS]' & '[EOS]'\n",
        "    bos_token = '[BOS]'\n",
        "    eos_token = '[EOS]'\n",
        "    updated_tokens = (\n",
        "        [bos_token]\n",
        "        + tokens\n",
        "        + [eos_token]\n",
        "    )\n",
        "    return updated_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5sByy-VVL36",
        "outputId": "4ebcee37-5407-4d27-daca-f9161d606992"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[BOS]', 'mr', '.', 'louis', 'continued', 'to', 'say', ',', '\"', 'penguins', 'are', 'important', ',', 'but', 'we', 'mustn', \"'\", 't', 'forget', 'the', 'nuumber', '1', 'priority', ':', 'the', 'reader', '!', '\"', '[EOS]']\n"
          ]
        }
      ],
      "source": [
        "# Test full pipeline (normalizing, pretokenizing, tokenizing, & postprocessing)\n",
        "tokens = tokenize_text(sample_text)\n",
        "tokens = postprocess_tokens(tokens)\n",
        "\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjcuzrCBVL36"
      },
      "source": [
        "# Encoding & Decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gnqu6UTVL36"
      },
      "source": [
        "## Encoding Text to Token IDs\n",
        "\n",
        "Create an encoder (`encode()`) that will encode the token strings to integer IDs\n",
        "by defining how to map each token to a unique ID.\n",
        "\n",
        "> HINT:\n",
        ">\n",
        "> An easy method is to assign an arbitrary integer to each unique token from\n",
        "> the corpus by iterating through the unique tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "yYziyWGwVL36"
      },
      "outputs": [],
      "source": [
        "# Sample corpus (normally this would be much bigger)\n",
        "sample_corpus = (\n",
        "    '''Mr. Louis continued to say, \"Penguins are important, \\nbut we mustn't forget the nuumber 1 priority: the READER!\"''',\n",
        "    '''BRUTUS:\\nHe's a lamb indeed, that baes like a bear.''',\n",
        "    '''Both by myself and many other friends:\\mBut he, his own affections' counsellor,\\nIs to himself--I will not say how true--\\nBut to himself so secret and so close,'''\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Methods for Updating Dictionaries\n",
        "\n",
        "Here are three methods to update a dictionary, specifically `token2id`, to include new entries from another dictionary.\n",
        "\n",
        "### 1. **In-Place Union Update (`|=`)**\n",
        "- **Syntax:** `token2id |= new_entries`\n",
        "- **Description:** Updates `token2id` in place by merging it with `new_entries` using the `|=` operator.\n",
        "- **Introduced in:** Python 3.9\n",
        "\n",
        "### 2. **Using `update()` Method**\n",
        "- **Syntax:** `token2id.update(new_entries)`\n",
        "- **Description:** Uses the `update()` method to add or update key-value pairs from `new_entries` into `token2id`.\n",
        "- **Common Use Case:** Directly modifies the original dictionary.\n",
        "\n",
        "### 3. **Reassignment with Unpacking (`{**token2id, **new_entries}`)**\n",
        "- **Syntax:** `token2id = {**token2id, **new_entries}`\n",
        "- **Description:** Merges `token2id` and `new_entries` by unpacking them into a new dictionary and reassigning the result back to `token2id`.\n",
        "- **Common Use Case:** Creates a new merged dictionary, leaving the original unchanged until reassignment.\n",
        "\n",
        "All three methods effectively merge dictionaries, but the choice depends on the Python version and whether you prefer modifying the dictionary in place or creating a new one."
      ],
      "metadata": {
        "id": "BTqMeLxYpy5l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "3XgMbWT1VL36"
      },
      "outputs": [],
      "source": [
        "# Create an encoder to transform token strings to IDs using the sample\n",
        "# corpus as the basis of your encoding\n",
        "\n",
        "# Your code here (might be outside of the encode() function scope)\n",
        "\n",
        "# Retrieve unique tokens (from the pipeline defined above) in a set\n",
        "unique_tokens = set()\n",
        "for text in sample_corpus:\n",
        "    tokens_from_text = tokenize_text(text)\n",
        "    tokens_from_text = postprocess_tokens(tokens_from_text)\n",
        "    unique_tokens.update(tokens_from_text)\n",
        "\n",
        "# Create mapping (dictionary) for unique tokens using arbitrary & unique IDs\n",
        "token2id = defaultdict(lambda : 0) # Allow for unknown tokens to map to 0\n",
        "token2id |= {\n",
        "    token: idx\n",
        "    for idx, token in enumerate(unique_tokens, 1) # Skip 0 (represents unknown)\n",
        "}\n",
        "\n",
        "# A mapping for IDs to convert back to token\n",
        "id2token = defaultdict(lambda : '[UNK]') # Allow for unknown token ('[UNK]')\n",
        "id2token |= {\n",
        "    idx: token\n",
        "    for token, idx in token2id.items()\n",
        "}\n",
        "\n",
        "\n",
        "def encode(tokens: list[str]) -> list[int]:\n",
        "    # Complete this function to encode tokens to integer IDs\n",
        "    encoded_tokens = [token2id[token] for token in tokens]\n",
        "    return encoded_tokens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyupYcmJVL36"
      },
      "source": [
        "### Test `encode()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1s-jRt6YVL36",
        "outputId": "361b7e8c-91b5-4e62-f0db-c168bba6d894"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:\n",
            "['[BOS]', 'mr', '.', 'louis', 'continued', 'to', 'say', ',', '\"', 'penguins', 'are', 'important', ',', 'but', 'we', 'mustn', \"'\", 't', 'forget', 'the', 'nuumber', '1', 'priority', ':', 'the', 'reader', '!', '\"', '[EOS]']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Use sample text for testing\n",
        "sample_text = sample_corpus[0]\n",
        "# Create tokens (to be fed to encode())\n",
        "tokens = tokenize_text(sample_text)\n",
        "tokens = postprocess_tokens(tokens)\n",
        "print(f'Tokens:\\n{tokens}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsI_y1DOVL36",
        "outputId": "9b4f1b7b-af45-4b5f-f785-403aa3a715bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Tokens:\n",
            "[36, 7, 24, 30, 53, 33, 6, 31, 17, 21, 49, 3, 31, 50, 44, 43, 56, 37, 20, 22, 29, 34, 55, 32, 22, 51, 4, 17, 28]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test encode()\n",
        "encoded_tokens = encode(tokens)\n",
        "print(f'Encoded Tokens:\\n{encoded_tokens}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yZ67ijjVL36"
      },
      "source": [
        "## Decoding Token IDs to Text\n",
        "\n",
        "Based on your enocder you created (`encode()`), create a decoder (`decode()`) to\n",
        "take a list of token IDs and map them to their associated token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "zGTjECWEVL37"
      },
      "outputs": [],
      "source": [
        "# COMPLETE: Create an encoder to transform IDs (from encode()) to token strings\n",
        "\n",
        "def decode(ids: list[int]) -> list[str]:\n",
        "    # Complete this function to decode integer IDs to token strings\n",
        "    token_strings = [id2token[idx] for idx in ids]\n",
        "    return token_strings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1djvFIlVVL37"
      },
      "source": [
        "### Test `decode()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ausc0smoVL37",
        "outputId": "1aac078b-d001-4fc1-c50c-3bd03fc843a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:\n",
            "['[BOS]', 'mr', '.', 'louis', 'continued', 'to', 'say', ',', '\"', 'penguins', 'are', 'important', ',', 'but', 'we', 'mustn', \"'\", 't', 'forget', 'the', 'nuumber', '1', 'priority', ':', 'the', 'reader', '!', '\"', '[EOS]']\n",
            "\n",
            "Encoded Tokens:\n",
            "[36, 7, 24, 30, 53, 33, 6, 31, 17, 21, 49, 3, 31, 50, 44, 43, 56, 37, 20, 22, 29, 34, 55, 32, 22, 51, 4, 17, 28]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Use sample text for testing\n",
        "sample_text = sample_corpus[0]\n",
        "# Create tokens\n",
        "tokens = tokenize_text(sample_text)\n",
        "tokens = postprocess_tokens(tokens)\n",
        "print(f'Tokens:\\n{tokens}\\n')\n",
        "\n",
        "# Create token IDs (to be fed to decode())\n",
        "encoded_tokens = encode(tokens)\n",
        "print(f'Encoded Tokens:\\n{encoded_tokens}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOUBBL-ZVL37",
        "outputId": "0ce5b16d-ebcd-4a74-dc53-db628c7ec693"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoded Tokens:\n",
            "['[BOS]', 'mr', '.', 'louis', 'continued', 'to', 'say', ',', '\"', 'penguins', 'are', 'important', ',', 'but', 'we', 'mustn', \"'\", 't', 'forget', 'the', 'nuumber', '1', 'priority', ':', 'the', 'reader', '!', '\"', '[EOS]']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test out decode()\n",
        "decoded_tokens = decode(encoded_tokens)\n",
        "print(f'Decoded Tokens:\\n{decoded_tokens}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ycus9-iKmq8O"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}